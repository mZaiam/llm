{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R1XZCCgZ_m-4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we are going to use a BERT-like model to create embeddings for words."
      ],
      "metadata": {
        "id": "vOIJkjJSEvmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")"
      ],
      "metadata": {
        "id": "K1xSDQSr_rxP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating the tokens."
      ],
      "metadata": {
        "id": "v9uvstSYE4co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer('hello world', return_tensors='pt')\n",
        "output = model(**tokens)[0]\n",
        "\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc082D4rADUm",
        "outputId": "26a444b4-693c-4dcd-cf16-0033b349380d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 384])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, each token was embedded into a tensor of dimension 384. Note that the final embeddings contain contextual information of their neighboors, as they are the outputs from a BERT model.\n",
        "\n",
        "We can also generate sentence embeddings, in similar fashion."
      ],
      "metadata": {
        "id": "x-3d1JdyFCEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "output = model.encode(\"sentence embedding\")\n",
        "\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kaP_twhB80y",
        "outputId": "0aff56de-2966-419a-9527-43a90efe39b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the sentence was embedded into a 768-shaped tensor."
      ],
      "metadata": {
        "id": "vPxNMrmpFfzy"
      }
    }
  ]
}
