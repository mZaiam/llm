{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7bzMsgjyr36c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers.pipelines.pt_utils import KeyDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_7Tdh3bv73q"
      },
      "source": [
        "Loading the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uhJ2CH9fxVtJ"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    trust_remote_code=True,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBcPelM4wClB"
      },
      "source": [
        "Analyzing the structure of the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o2hXPtZ6wvLM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|user|>\n",
            "Tell me a joke about chickens.<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "msg = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Tell me a joke about chickens.\"\n",
        "}]\n",
        "\n",
        "prompt = pipe.tokenizer.apply_chat_template(msg, tokenize=False)\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxp5hqsGwQFx"
      },
      "source": [
        "It is possible to see that there is a `role` and a `content` flag. These can be modified to extract a more precise behaviour.\n",
        "\n",
        "If one wishes to obtain more variations in the output, the temperature parameter can be used. As it gets higher, the distribution of next words gets more uniform, causing more diverse outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NJMzJufQcFfy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Why did the farmer throw his egg instead of eating it? Because it was still chicken!\n"
          ]
        }
      ],
      "source": [
        "output = pipe(msg, do_sample=True, temperature=1)\n",
        "\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEEsUGYFW8ri"
      },
      "source": [
        "For some tasks, the user can provide some examples of prompt-outputs for the LLM. This is referred as In-Context Learning. Here, the model can \"infer\" how to use a word it has never seen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E5FM9PrTDtNQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " During the medieval reenactment, the knight skillfully screeged the wooden target with precision and grace.\n"
          ]
        }
      ],
      "source": [
        "one_shot_prompt = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"A 'Gigamuru' is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:\"\n",
        "    }\n",
        "]\n",
        "\n",
        "outputs = pipe(one_shot_prompt)\n",
        "print(outputs[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNGrCWevX3p5"
      },
      "source": [
        "One can also break the desired task into smaller sub-problems. For that, the output of the LLM can be included as the input for a new prompt, saving resources and potentially getting better results. This is referred as Chain-Prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oWbTgZW_W3Gr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Name: FitAI\n",
            "Slogan: \"Transforming Gym Data into Actionable Insights with AI\"\n",
            "\n",
            " \"Elevate your fitness journey with FitAI! Our cutting-edge AI technology takes your gym data and transforms it into actionable insights, helping you achieve your fitness goals faster and smarter. Say goodbye to guesswork and hello to personalized workout plans that adapt to your progress. FitAI â€“ where AI meets fitness!\"\n"
          ]
        }
      ],
      "source": [
        "product_prompt = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Create a name and slogan for a chatbot based on LLMs for analyzing gym data.\"}\n",
        "]\n",
        "outputs = pipe(product_prompt)\n",
        "product_description = outputs[0][\"generated_text\"]\n",
        "\n",
        "print(product_description)\n",
        "\n",
        "print()\n",
        "\n",
        "sales_prompt = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Generate a very short sales pitch for the following product: '{product_description}'\"\n",
        "    }\n",
        "]\n",
        "\n",
        "outputs = pipe(sales_prompt)\n",
        "sales_pitch = outputs[0][\"generated_text\"]\n",
        "\n",
        "print(sales_pitch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcIGaHPxo001"
      },
      "source": [
        "Another broad area of Prompt-Engineering is Reasoning. Here, the focus is to somehow make the LLM think about the anwser before actually giving the final output. Superficially, this can be achieved through a well structured prompt, which can create a Chain-of-Thought behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lGCtB8gFo3Am"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Step 1: The cafeteria starts with 23 apples.\n",
            "Step 2: They used 20 apples to make lunch, so we subtract 20 from the initial amount: 23 - 20 = 3 apples remaining.\n",
            "Step 3: The cafeteria bought 6 more apples, so we add 6 to the remaining amount: 3 + 6 = 9 apples.\n",
            "\n",
            "The cafeteria now has 9 apples.\n"
          ]
        }
      ],
      "source": [
        "zeroshot_cot_prompt = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let's think step-by-step.\"}\n",
        "]\n",
        "\n",
        "outputs = pipe(zeroshot_cot_prompt)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3dE4xu2qBuu"
      },
      "source": [
        "Another Reasoning approach is to leverage severall outputs from the LLM. This is an expensive technique, as the model needs to process the output more than once. This can be simulated through a single prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A0WmVeGyqD-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Expert 1:\n",
            "Step 1: Start with the initial number of apples, which is 23.\n",
            "\n",
            "Expert 2:\n",
            "Step 1: Subtract the number of apples used for lunch, which is 20.\n",
            "Step 2: Add the number of apples bought, which is 6.\n",
            "\n",
            "Expert 3:\n",
            "Step 1: Start with the initial number of apples, which is 23.\n",
            "Step 2: Subtract the number of apples used for lunch, which is 20.\n",
            "Step 3: Add the number of apples bought, which is 6.\n",
            "\n",
            "Results:\n",
            "All three experts arrived at the same answer:\n",
            "\n",
            "Expert 1: 23 - 20 + 6 = 9 apples\n",
            "Expert 2: (23 - 20) + 6 = 9 apples\n",
            "Expert 3: (23 - 20) + 6 = 9 apples\n",
            "\n",
            "All three experts agree that the cafeteria has 9 apples left.\n"
          ]
        }
      ],
      "source": [
        "zeroshot_tot_prompt = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "outputs = pipe(zeroshot_tot_prompt)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
